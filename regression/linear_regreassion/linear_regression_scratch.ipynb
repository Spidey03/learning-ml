{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression analysis is a form of __predictive modelling__ technique which investigates the relationship between a _dependent_ variable and _independent_ variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- The data is modelled using a straight line called __trend line__\n",
    "- Used with _Continuous variables_\n",
    "- Accuracy can be measured by $R^2$, Adjusted $R^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,10,50)\n",
    "y = 2*x+1\n",
    "\n",
    "plt.plot(x, y, '-r', label='y=2x+1')\n",
    "plt.xlabel(r'independent variable $\\longrightarrow$', color='#1C2833')\n",
    "plt.ylabel(r'dependent variable $\\longrightarrow$', color='#1C2833')\n",
    "plt.legend(loc='upper left')\n",
    "x1 = [-5, -2, 0, 1, 4, 6, 8, 9]\n",
    "y1 = [-10, -0, 1, 2, 12, 10, 17, 20]\n",
    "plt.scatter(x1, y1, label='actual')\n",
    "x2 = [-5, -2, 0, 1, 4, 6, 8, 9]\n",
    "y2 = [-9, -3, 1, 3, 9, 13, 17, 19]\n",
    "plt.scatter(x2, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ The input variables are referred to as an __independent variables__ or __predictors__\n",
    "\n",
    "$\\Rightarrow$ The output variabales are referred as an __dependent variables__ or __target variables__\n",
    "\n",
    "### $ Y = mX + c $\n",
    "\n",
    "Y $\\rightarrow$ is dependent variable $\\\\$\n",
    "X $\\rightarrow$ is independent variable $\\\\$\n",
    "m $\\rightarrow$ is _regression coefficient_ or _slope of line_ $\\\\$\n",
    "c $\\rightarrow$ is _intercept_ of line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error term__\n",
    "### $$ Y = mX + c + \\epsilon $$\n",
    "\n",
    "$\\epsilon$ $\\rightarrow$ Residual of actual and predicted values\n",
    "\n",
    "c $\\rightarrow$ denotes the values of Y when X is `zero`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "$$ h(\\theta) = \\theta_0 + \\theta_1x_1+ \\cdots + \\theta_nx_n $$\n",
    "$$ \\Rightarrow h(\\theta) = \\theta^Tx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (MSE)\n",
    "Measures difference for the actual and predicted values.$\\\\$\n",
    "If MSE is lower, then the model is _best fit_\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m{(h_\\theta(x^i) - y^i)^2} $$\n",
    "\n",
    "$$ m \\ is \\ no.of \\ observations $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "- Lasso Regression uses L1 regularization technique. $\\\\$\n",
    "- Sum of absolute values of the coefficients (L1 Norm)\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m{(h_\\theta(x^i) - y^i)^2} + \\frac{\\lambda}{2m} \\sum_{j=1}^n\\theta_j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "- Ridge Regression uses L2 regularization technique\n",
    "- Sum of the squares of coefficients (L2 Norm)\n",
    "$$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m{(h_\\theta(x^i) - y^i)^2} + \\frac{\\lambda}{2m} \\sum_{j=1}^n\\theta_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda$ is too small $\\\\$\n",
    "$\\rightarrow$ $\\lambda$ \\approx 0 $\\Rightarrow$ J \\approx MSE $\\\\$\n",
    "$\\rightarrow$ No effect in regularization $\\\\$\n",
    "$\\rightarrow$ Leads to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda$ is too large $\\\\$\n",
    "$\\rightarrow$ MSE + $\\lambda||\\theta||^2$ = $J(\\theta)$ $\\\\$\n",
    "$\\rightarrow$ $\\theta_0 \\approx 0, \\cdots ,\\theta_n \\approx 0$ $\\\\$\n",
    "$\\rightarrow$ Leads to underfitting $\\\\$\n",
    "$\\rightarrow$ $h_{\\theta}x \\approx \\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent\n",
    "It is an optimazation algorithm used to minimize the function by iteratively movinng in direction of __steepest decent__.\n",
    "\n",
    "### Repeat until convergence \n",
    "### $\\theta^{t+1} := \\theta^t - \\alpha\\frac{dJ}{d\\theta}$\n",
    "\n",
    "\n",
    "### $$\\theta_{optimal} = argminJ(\\theta)$$\n",
    "### $$\\theta^{t+1} = \\theta^t - \\alpha\\frac{dJ}{d\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\theta_0, \\theta_1, \\cdots, \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^{m}(\\theta_0 + \\theta_1x_1 + \\cdots + \\theta_nx_n - y^i)^2 $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta_0, \\theta_1, \\cdots, \\theta_n)}{\\partial \\theta_0} = \\frac{1}{m}\\sum_{i=1}{m}(\\theta_0 + \\theta_1x_1 + \\cdots + \\theta_nx_n - y^i) $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta_0, \\theta_1, \\cdots, \\theta_n)}{\\partial \\theta_i} = \\frac{1}{m}\\sum_{i=1}{m}(\\theta_0 + \\theta_1x_1 + \\cdots + \\theta_nx_n - y^i).x^i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For single instances, defining $x_0$ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$$ h = np.dot(theta.T, X) $$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple instances,\n",
    "__$$ h = np.dot(X, \\theta) $$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss = h_\\theta(x^i) - y^i $$\n",
    "\n",
    "$$ \\Rightarrow X\\theta - Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta = \\theta - \\frac{\\alpha}{m}.X^T(X\\theta-Y) + \\lambda I'\\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m{(h_\\theta(x^i) - y^i)^2} $$\n",
    "$$ \\Rightarrow \\frac{1}{2m} (X\\theta-Y)^T(X\\theta-Y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ if $\\alpha$ value is low then, the convergence is very slow. $\\\\$\n",
    "$\\Rightarrow$ optimal value for \\alpha is `1` $\n",
    "\n",
    "$\\rightarrow$ start $\\alpha$ with small random value $\\\\$\n",
    "$\\rightarrow$ plot the graph #no.of. iterations vs $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ if $J(\\theta)$ is oscilating then decrease $\\alpha \\downarrow \\\\$\n",
    "$\\Rightarrow$ if $J(\\theta)$ is kept increasing then choose small $\\alpha$ value $\\\\$\n",
    "$\\Rightarrow$ else change $\\alpha$ by multiplying by `3` $\\\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Eqn Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\theta) = \\frac{1}{2m} (X\\theta-Y)^T(X\\theta-Y) $$\n",
    "$$ after\\;some\\;series\\;of\\;steps... $$\n",
    "\n",
    "$$ \\theta = (X^TX)^{-1}X^TY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_: $ X^TX $ is invertible when $\\\\$\n",
    "$ \\Rightarrow$ there are reduntant features.$\\\\$\n",
    "$ \\Rightarrow$ too many features (m <= n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_2$ regularizion $\\\\$\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}(X\\theta - Y)^T(X\\theta-Y) + \\frac{\\lambda}{2m}\\theta^TI'\\theta\\\\$\n",
    "\n",
    "where $I'$ is an identity matrix with the first element as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func_for_ridge_regression(X, Y, theta, Lambda):\n",
    "    m = X.shape[0] # size of dataset\n",
    "    H = np.dot(X, theta) # hypothesis\n",
    "    loss = H - Y\n",
    "    no_of_parameters = len(theta)\n",
    "    I =  np.eye(no_of_parameters, dtype=int)\n",
    "    I[0][0] = 0\n",
    "    regularization = (Lambda/(2*m)) * np.dot(np.dot(theta.T, I), theta)\n",
    "    cost = 1/(2*m) * np.dot(loss.T, loss) + regularization\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent_of_cost_function(X, Y, theta, Lambda):\n",
    "    m = X.shape[0]\n",
    "    H = np.dot(X, theta)\n",
    "    loss = H - Y\n",
    "    \n",
    "    no_of_parameters = len(theta)\n",
    "    I =  np.eye(no_of_parameters, dtype=int)\n",
    "    I[0][0] = 0\n",
    "    regularization = Lambda * np.dot(I, theta)\n",
    "    \n",
    "    theta = 1/m * np.dot(X.T, loss) + regularization\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent_of_ridge_regression(X, Y, theta, alpha, Lambda, cost_diff_threshold):\n",
    "    i = 0\n",
    "    cost_diff = cost_diff_threshold + 1\n",
    "    thetas = [theta]\n",
    "    costs = [cost_func_for_ridge_regression(X=X, Y=Y, theta=theta, Lambda=Lambda)]\n",
    "    while cost_diff > cost_diff_threshold:\n",
    "        d_theta = gradient_decent_of_cost_function(X=X, Y=Y, theta=theta, Lambda=Lambda)\n",
    "        theta = theta - (alpha * d_theta)\n",
    "        thetas.append(theta)\n",
    "        costs.append(cost_func_for_ridge_regression(X=X, Y=Y))\n",
    "        cost_diff = costs[i+1] - costs[i]\n",
    "        if cost_diff > 0:\n",
    "            print(f\"Diverging for theta : {theta}\")\n",
    "            break\n",
    "        i += 1\n",
    "    return thetas, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_eqn_ridge_eqn(X, Y, Lambda):\n",
    "    XT_X = np.dot(X.T, X)\n",
    "    XT_Y = np.dot(X.T, Y)\n",
    "\n",
    "    no_of_parameters = len(X[0])\n",
    "    I =  np.eye(no_of_parameters, dtype=int)\n",
    "    I[0][0] = 0\n",
    "    inverse = np.linalg.pinv(XT_X, Lambda*I)\n",
    "    \n",
    "    result = np.dot(inverse, XT_Y)\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predicted_Y, actual_Y):\n",
    "    difference = actual_Y - predicted_Y\n",
    "    m = len(predicted_Y)\n",
    "    mse = (1/m)*np.dot(difference.T, difference)\n",
    "    return np.squeeze(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_theta(num_parameters):\n",
    "    theta = np.random.rand(num_parameters,1)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient_decent(train_X, train_Y, Lambda):\n",
    "    initial_theta = get_initial_theta(train_X.shape[1])\n",
    "    cost_diff_threshold = 1e-6\n",
    "    learning_rate = 0.1\n",
    "    thetas, costs = gradient_decent_of_ridge_regression(\n",
    "        X=train_X,\n",
    "        Y=train_Y,\n",
    "        theta=initial_theta,\n",
    "        alpha=learning_rate,\n",
    "        Lambda=Lambda,\n",
    "        cost_diff_threshold=cost_diff_threshold\n",
    "    )\n",
    "    theta_GD = thetas[-1]\n",
    "    return theta_GD, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_X, data_Y, theta):\n",
    "    predicted_Y = np.dot(data_X.T, theta)\n",
    "    mse = MSE(predicted_Y=predicted_Y, actual_Y=data_Y)\n",
    "    return mse, predicted_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(train_X, train_Y, Lambda, split_data=True, use_normal_eqn=True):\n",
    "    train_X = np.insert(train_X, 0, 1, axis=1)\n",
    "    train_Y = train_Y.reshape((-1, 1))\n",
    "\n",
    "    if use_normal_eqn:\n",
    "        theta = normal_eqn_ridge_eqn(X=train_X, Y=train_Y, Lambda=Lambda)\n",
    "    else:\n",
    "        theta, costs = apply_gradient_decent(train_X=train_X, train_Y=train_Y, Lambda=Lambda)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
